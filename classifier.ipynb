{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 102Flowers Image Classifier\n",
    "\n",
    "This is the main notebook for the project. See the associated report (WIP) for more information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK IN PROGRESS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set hyperparameters.\n",
    "training_batch_size = 20\n",
    "validation_batch_size = 20\n",
    "test_batch_size = 20\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "#momentum = 0.9\n",
    "crop_size = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs. Using cuda:0.\n"
     ]
    }
   ],
   "source": [
    "# Default to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Switch to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "\tprint(f\"Found {torch.cuda.device_count()} GPUs. Using cuda:0.\")\n",
    "\tdevice = torch.device(\"cuda:0\")\n",
    "else:\n",
    "\tprint(\"No GPUs found, using CPU.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "validation_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"val\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "testing_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"test\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=training_batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=validation_batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 1]], device='cuda:0'),\n",
       " torch.Size([102, 102]))"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = F.one_hot(torch.tensor([e for e in range(0,102)], device=\"cuda:0\"), num_classes=102)\n",
    "classifications, classifications.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F102Classifier(nn.Module):\n",
    "    \n",
    "\tdef __init__(self):\n",
    "\t\tsuper(F102Classifier, self).__init__()\n",
    "\t\t\n",
    "\t\tself.pool = nn.MaxPool2d(8, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3) #3 inputs 6 hiddens\n",
    "\t\tself.BatchNorm = nn.BatchNorm2d(6) #Normalizes above\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3) # 12 hiddens\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3) # 24 hiddens\n",
    "\t\tself.fc1 = nn.Linear(15000, 102) #102 output neurons\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = (F.relu(self.conv1(x)))\n",
    "\t\t#print(\"after conv1: \", x.size())\n",
    "\t\tx = self.BatchNorm(x)\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\t#print(\"after conv2: \", x.size())\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\t#print(\"after conv3: \", x.size())\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\t#print(\"after view: \", x.size())\n",
    "\t\tx = self.fc1(x)\n",
    "\t\t#print(\"after fc1: \", x.size())\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "net = F102Classifier()\n",
    "if device == torch.device(\"cuda:0\"):\n",
    "\tnet.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function & Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "\tepoch = 1\n",
    "\tfor batch, (i,j) in enumerate(dataloader):\n",
    "\t\tfeatures, labels = i.to(device), j.to(device)\n",
    "  \n",
    "\t\t# Compute the loss based off the predictions vs labels\n",
    "\t\tpredictions = model(features)\n",
    "\t\tloss = loss_fn(predictions, labels)\n",
    "\n",
    "\t\t#Compute back propagation\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif (batch+1) % 51 == 0:\n",
    "\t\t\tprint(f'Average Loss in Epoch: {loss.item()}')\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    for batch, (i,j) in enumerate(dataloader):\n",
    "        features, labels = i.to(device), j.to(device)\n",
    "        model.cuda()\n",
    "        pred = model(features)\n",
    "        test_loss += loss_fn(pred, labels).item()\n",
    "        correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss : {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1-------------\n",
      "Average Loss in Epoch: 4.626456260681152\n",
      "Epoch 2-------------\n",
      "Average Loss in Epoch: 4.7379350662231445\n",
      "Epoch 3-------------\n",
      "Average Loss in Epoch: 3.9942638874053955\n",
      "Epoch 4-------------\n",
      "Average Loss in Epoch: 3.6112289428710938\n",
      "Epoch 5-------------\n",
      "Average Loss in Epoch: 2.247495412826538\n",
      "Epoch 6-------------\n",
      "Average Loss in Epoch: 2.269381284713745\n",
      "Epoch 7-------------\n",
      "Average Loss in Epoch: 0.7683330774307251\n",
      "Epoch 8-------------\n",
      "Average Loss in Epoch: 0.289736807346344\n",
      "Epoch 9-------------\n",
      "Average Loss in Epoch: 0.21078446507453918\n",
      "Epoch 10-------------\n",
      "Average Loss in Epoch: 0.09614106267690659\n",
      "Epoch 11-------------\n",
      "Average Loss in Epoch: 0.06913961470127106\n",
      "Epoch 12-------------\n",
      "Average Loss in Epoch: 0.06997961550951004\n",
      "Epoch 13-------------\n",
      "Average Loss in Epoch: 0.04501510411500931\n",
      "Epoch 14-------------\n",
      "Average Loss in Epoch: 0.0034210074227303267\n",
      "Epoch 15-------------\n",
      "Average Loss in Epoch: 0.0010489567648619413\n",
      "Epoch 16-------------\n",
      "Average Loss in Epoch: 0.004766765050590038\n",
      "Epoch 17-------------\n",
      "Average Loss in Epoch: 0.02471364662051201\n",
      "Epoch 18-------------\n",
      "Average Loss in Epoch: 0.1824718862771988\n",
      "Epoch 19-------------\n",
      "Average Loss in Epoch: 0.0961228683590889\n",
      "Epoch 20-------------\n",
      "Average Loss in Epoch: 0.005736852530390024\n",
      "Test Error: \n",
      " Accuracy: 12.9%, Avg loss : 9.751864 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}-------------')\n",
    "    train(train_dataloader, net, loss_function, optimiser)\n",
    "\n",
    "test(validation_dataloader, net, loss_function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 30,0.001 = 15.2%\n",
    " 100, 0.001 = 14.3%\n",
    " 50, 0.001 = 16.4%\n",
    " 30,0.01 = 1.0%  Herma-OF\"\"\"\n",
    "\n",
    "\"\"\"For Theo\t\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3) \n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.conv4 = nn.Conv2d(24, 48, 3)\n",
    "\t\tself.conv5 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(384, 1024)\n",
    "\t\tself.fc2 = nn.Linear(1024, 512)\n",
    "\t\tself.fc3 = nn.Linear(512, 102)\t\n",
    "30, 0.01 = 1.0%\n",
    "30, 0.001 = 14.0%\n",
    "100, 0.001 = 13.2%\n",
    "30, 0.0001 = 7.2%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 12, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(12, 48, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(18816, 1024)\n",
    "\t\tself.fc2 = nn.Linear(1024, 512)\n",
    "\t\tself.fc3 = nn.Linear(512, 102)\t\t\n",
    "100, 0.001 = 9.5%\n",
    "30, 0.001 = 17.8%\n",
    "50, 0.001 = 1.0%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 12, 3)\n",
    "\t\tself.fc1 = nn.Linear(47628, 102)\n",
    "30, 0.001 = 19.9%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.fc1 = nn.Linear(23814, 102)\n",
    "30, 0.001 = 17.9%\n",
    "15, 0.001 = 16.0%\n",
    "10, 0.001 = 19.0%\n",
    "5, 0.001 = 14.4%\n",
    "\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.fc1 = nn.Linear(23814, 102)\n",
    "30, 0.001 = 19.2%\n",
    "10, 0.001 = 16.8%\n",
    "50, 0.001 = 17.0\n",
    "\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.conv4 = nn.Conv2d(24, 48, 3)\n",
    "\t\tself.conv5 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(384, 102)\n",
    "30, 0.001 = 18.6%\n",
    "\n",
    "MORE\n",
    "self.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.fc1 = nn.Linear(21600, 102)\n",
    "\tdef forward(self, x):\n",
    "\t\tx = (F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = self.fc1(x)\n",
    "        5 epochs = 10.5%\n",
    "        10 epochs = 17.2%\n",
    "        30 epochs = 16.1%\n",
    "        15 epochs = 10.9\n",
    "        \n",
    "        Same as above but kernel size 4 and self.fc1 = nn.Linear(18816, 102)\n",
    "        10 epochs = 9.2%\n",
    "        15 epochs = 19.2%\n",
    "        20 epochs = 21.5%\n",
    "        25 epochs = 14.5%\n",
    "        30 epochs = 18.4%\n",
    "        \n",
    "        Same as above burt kernal size 8 and self.fc1 = nn.Linear(15000, 102)\n",
    "        20 epochs = 15.4 %\n",
    "        30 epochs = 25.5%\n",
    "        35 epochs = 16.1%\n",
    "        \n",
    "        Same as above but with batch normilization after conv1\n",
    "        20 epochs = 12.9%\n",
    "        25 epochs = 25.0%\n",
    "        30 epochs = 19.6%\n",
    "        40 epochs = 18.0%\n",
    " \"\"\"\n",
    "save_path = \"./models/classifier.pth\"\n",
    "torch.save(net.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
