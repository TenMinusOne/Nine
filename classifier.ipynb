{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "metadata": {
        "id": "AsR-kxmSeHP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Set hyperparameters.\n",
        "training_batch_size = 20\n",
        "validation_batch_size = 20\n",
        "test_batch_size = 102\n",
        "epochs = 30\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "crop_size = 227"
      ],
      "metadata": {
        "id": "SA3I5HZPeYh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydZj4Dr1defq",
        "outputId": "1643d28c-6b2a-4a63-8d3f-f99038e13175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 GPUs. Using cuda:0.\n"
          ]
        }
      ],
      "source": [
        "# Default to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Switch to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "\tprint(f\"Found {torch.cuda.device_count()} GPUs. Using cuda:0.\")\n",
        "\tdevice = torch.device(\"cuda:0\")\n",
        "else:\n",
        "\tprint(\"No GPUs found, using CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.Flowers102(\n",
        "    root = \"data\",\n",
        "    split = \"train\",\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(crop_size),\n",
        "        transforms.CenterCrop(crop_size),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "training_data2 = datasets.Flowers102(\n",
        "    root = \"data\",\n",
        "    split = \"train\",\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(crop_size),\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.CenterCrop(crop_size),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "training_data3 = datasets.Flowers102(\n",
        "    root = \"data\",\n",
        "    split = \"train\",\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(crop_size),\n",
        "        transforms.RandomRotation(degrees=45),\n",
        "        transforms.CenterCrop(crop_size),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "\n",
        "training_data4 = datasets.Flowers102(\n",
        "    root = \"data\",\n",
        "    split = \"train\",\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(crop_size),\n",
        "        transforms.RandomRotation(degrees=23),\n",
        "        transforms.CenterCrop(crop_size),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "\n",
        "validation_data = datasets.Flowers102(\n",
        "    root = \"data\",\n",
        "    split = \"val\",\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(crop_size),\n",
        "        transforms.CenterCrop(crop_size),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "testing_data = datasets.Flowers102(\n",
        "    root = \"data\",\n",
        "    split = \"test\",\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(crop_size),\n",
        "        transforms.CenterCrop(crop_size),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    download=True\n",
        ")"
      ],
      "metadata": {
        "id": "zbg87YxKeRJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizontal = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=1)\n",
        "])\n",
        "\n",
        "degrees = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=45)\n",
        "])"
      ],
      "metadata": {
        "id": "1batnwFAeSMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_data = ConcatDataset([training_data, training_data2, training_data3, training_data4])"
      ],
      "metadata": {
        "id": "EKeUBowZvl6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(augmented_data, batch_size=training_batch_size, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size=validation_batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(testing_data, batch_size=test_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "iOyPOi-VeZVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class F102Classifier(nn.Module):\n",
        "    \n",
        "\tdef __init__(self):\n",
        "\t\tsuper(F102Classifier, self).__init__()\n",
        "\t\tself.features = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(3, 27, 3, 1, 1),\n",
        "\t\t\tnn.BatchNorm2d(27),\n",
        "      nn.ReLU(),\n",
        "\t\t\tnn.MaxPool2d(2, 2),\n",
        "   \t\tnn.Conv2d(27, 81, 3, 1, 1),\n",
        "      nn.BatchNorm2d(81),\n",
        "\t\t\tnn.ReLU(),\n",
        "   \t\tnn.Conv2d(81, 243, 3, 1, 1),\n",
        "\t\t\tnn.BatchNorm2d(243),\n",
        "      nn.ReLU(),\n",
        "\t\t\tnn.MaxPool2d(2, 2),\n",
        "\t\t)\n",
        "  \n",
        "\t\tself.classifier = nn.Sequential(\n",
        "\t\t\tnn.Dropout(0.5),\n",
        "\t\t\tnn.Linear(243*56*56, 512),\n",
        "\t\t\tnn.BatchNorm1d(512),\n",
        "\t\t\tnn.ReLU(),\n",
        "   \t\tnn.Dropout(0.5),\n",
        "\t\t\tnn.Linear(512,102)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.features(x)\n",
        "\t\tx = torch.flatten(x)\n",
        "\t\tx = x.view(training_batch_size, -1)\n",
        "\t\tx = self.classifier(x)\n",
        "\t\treturn x\n",
        "\n",
        "net = F102Classifier()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    net.cuda()"
      ],
      "metadata": {
        "id": "3WMBRK8del5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "-64Se-bNefIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "\trunning_loss = 0.0\n",
        "\tfor batch, (i,j) in enumerate(dataloader):\n",
        "\t\tfeatures, labels = i.to(device), j.to(device)\n",
        "  \n",
        "\t\t# Compute the loss based off the predictions vs labels\n",
        "\t\tpredictions = model(features)\n",
        "\t\tloss = loss_fn(predictions, labels)\n",
        "\t\trunning_loss += loss.item()\n",
        "  \n",
        "\t\t#Compute back propagation\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tdivisor = batch\n",
        "\n",
        "\trunning_loss /= divisor\n",
        "\t\n",
        "\tprint(f'Average Loss in Epoch: {running_loss}')\n",
        "\t\t\t\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXh6WHxeeiW-",
        "outputId": "93be32c0-e340-4498-a1de-d746c21c5399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for batch, (i,j) in enumerate(dataloader):\n",
        "        features, labels = i.to(device), j.to(device)\n",
        "        pred = model(features)\n",
        "        test_loss += loss_fn(pred, labels).item()\n",
        "        correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss : {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "2FIFm0M6esNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f'Epoch {t+1}-------------')\n",
        "    train(train_dataloader, net, loss_function, optimiser)\n",
        "    test(validation_dataloader, net, loss_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VALiRo_e_Mh",
        "outputId": "a7223750-c43e-4064-a23d-1aff837b940e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1-------------\n",
            "Average Loss in Epoch: 3.4832298315217343\n",
            "Test Error: \n",
            " Accuracy: 24.6%, Avg loss : 3.015719 \n",
            "\n",
            "Epoch 2-------------\n",
            "Average Loss in Epoch: 1.6499384136622763\n",
            "Test Error: \n",
            " Accuracy: 29.8%, Avg loss : 3.277803 \n",
            "\n",
            "Epoch 3-------------\n",
            "Average Loss in Epoch: 0.5382211414977834\n"
          ]
        }
      ]
    }
  ]
}