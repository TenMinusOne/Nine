{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 102Flowers Image Classifier\n",
    "\n",
    "This is the main notebook for the project. See the associated report (WIP) for more information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK IN PROGRESS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set hyperparameters.\n",
    "training_batch_size = 20 #1020\n",
    "validation_batch_size = 20 #1020\n",
    "test_batch_size = 43 #6149\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "#momentum = 0.9\n",
    "crop_size = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs. Using cuda:0.\n"
     ]
    }
   ],
   "source": [
    "# Default to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Switch to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "\tprint(f\"Found {torch.cuda.device_count()} GPUs. Using cuda:0.\")\n",
    "\tdevice = torch.device(\"cuda:0\")\n",
    "else:\n",
    "\tprint(\"No GPUs found, using CPU.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "h_flipped_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.99),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "v_flipped_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(0.99),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "pos90_rotate_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomRotation([89,91]),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "minus90_rotate_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomRotation([-91,-89]),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "augmented_training_data = ConcatDataset([original_training_data,h_flipped_training_data,v_flipped_training_data,pos90_rotate_training_data,minus90_rotate_training_data])\n",
    "\n",
    "validation_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"val\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "testing_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"test\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(augmented_training_data, batch_size=training_batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=validation_batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 1]], device='cuda:0'),\n",
       " torch.Size([102, 102]))"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = F.one_hot(torch.tensor([e for e in range(0,102)], device=\"cuda:0\"), num_classes=102)\n",
    "classifications, classifications.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F102Classifier(nn.Module):\n",
    "    \n",
    "\tdef __init__(self):\n",
    "\t\tsuper(F102Classifier, self).__init__()\n",
    "\t\t\n",
    "\t\tself.pool = nn.MaxPool2d(8, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3) #3 inputs 6 hiddens\n",
    "\t\tself.batchnorm1 = nn.BatchNorm2d(6) #Normalizes above \n",
    "\t\tself.conv2 = nn.Conv2d(6, 30, 3) # 12 hiddens\n",
    "\t\tself.batchnorm2 = nn.BatchNorm2d(30)\n",
    "\t\tself.conv3 = nn.Conv2d(30, 30, 9) # 12 hiddens\n",
    "\t\tself.batchnorm3 = nn.BatchNorm2d(30)\n",
    "\t\tself.fc1 = nn.Linear(1080, 204)\n",
    "\t\tself.dropout = nn.Dropout(0.2)\n",
    "\t\tself.fc2 = nn.Linear(204, 102) #102 output neurons\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.batchnorm1(x)\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.batchnorm2(x)\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = self.batchnorm3(x)\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "\n",
    "net = F102Classifier()\n",
    "if device == torch.device(\"cuda:0\"):\n",
    "\tnet.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function & Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "\tepoch = 1\n",
    "\tfor batch, (i,j) in enumerate(dataloader):\n",
    "\t\tfeatures, labels = i.to(device), j.to(device)\n",
    "  \n",
    "\t\t# Compute the loss based off the predictions vs labels\n",
    "\t\tpredictions = model(features)\n",
    "\t\tloss = loss_fn(predictions, labels)\n",
    "\n",
    "\t\t#Compute back propagation\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif (batch+1) % len(dataloader) == 0: #after final batch of each epoch\n",
    "\t\t\tprint(f'Average Training Loss: {loss.item()}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    for batch, (i,j) in enumerate(dataloader):\n",
    "        features, labels = i.to(device), j.to(device)\n",
    "        model.cuda()\n",
    "        pred = model(features)\n",
    "        test_loss += loss_fn(pred, labels).item()\n",
    "        correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss : {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1-------------\n",
      "Average Training Loss: 4.648192882537842\n",
      "\n",
      "Validation Test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[421], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[39m#print(\"Training Test\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39m#test(train_dataloader, net, loss_function)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation Test\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     test(validation_dataloader, net, loss_function)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished Training and Testing\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[420], line 7\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      6\u001b[0m test_loss, correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m batch, (i,j) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      8\u001b[0m     features, labels \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39mto(device), j\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\Theod\\OneDrive\\Documents\\GitHub\\Nine\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Theod\\OneDrive\\Documents\\GitHub\\Nine\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Theod\\OneDrive\\Documents\\GitHub\\Nine\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Theod\\OneDrive\\Documents\\GitHub\\Nine\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Theod\\OneDrive\\Documents\\GitHub\\Nine\\.venv\\Lib\\site-packages\\torchvision\\datasets\\flowers102.py:81\u001b[0m, in \u001b[0;36mFlowers102.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m     80\u001b[0m     image_file, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image_files[idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_labels[idx]\n\u001b[1;32m---> 81\u001b[0m     image \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39;49mImage\u001b[39m.\u001b[39;49mopen(image_file)\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m     84\u001b[0m         image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\Theod\\OneDrive\\Documents\\GitHub\\Nine\\.venv\\Lib\\site-packages\\PIL\\Image.py:945\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    943\u001b[0m         mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mor\u001b[39;00m (mode \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m matrix):\n\u001b[1;32m--> 945\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    947\u001b[0m \u001b[39mif\u001b[39;00m matrix:\n\u001b[0;32m    948\u001b[0m     \u001b[39m# matrix conversion\u001b[39;00m\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Theod\\OneDrive\\Documents\\GitHub\\Nine\\.venv\\Lib\\site-packages\\PIL\\Image.py:1202\u001b[0m, in \u001b[0;36mImage.copy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \u001b[39mCopies this image. Use this method if you wish to paste things\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m \u001b[39minto an image, but still retain the original.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[39m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[1;32m-> 1202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mcopy())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}-------------')\n",
    "    train(train_dataloader, net, loss_function, optimiser)\n",
    "    #print(\"Training Test\")\n",
    "    #test(train_dataloader, net, loss_function)\n",
    "    print(\"Validation Test\")\n",
    "    test(validation_dataloader, net, loss_function)\n",
    "print('Finished Training and Testing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 30,0.001 = 15.2%\n",
    " 100, 0.001 = 14.3%\n",
    " 50, 0.001 = 16.4%\n",
    " 30,0.01 = 1.0%  Herma-OF\"\"\"\n",
    "\n",
    "\"\"\"For Theo\t\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3) \n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.conv4 = nn.Conv2d(24, 48, 3)\n",
    "\t\tself.conv5 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(384, 1024)\n",
    "\t\tself.fc2 = nn.Linear(1024, 512)\n",
    "\t\tself.fc3 = nn.Linear(512, 102)\t\n",
    "30, 0.01 = 1.0%\n",
    "30, 0.001 = 14.0%\n",
    "100, 0.001 = 13.2%\n",
    "30, 0.0001 = 7.2%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 12, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(12, 48, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(18816, 1024)\n",
    "\t\tself.fc2 = nn.Linear(1024, 512)\n",
    "\t\tself.fc3 = nn.Linear(512, 102)\t\t\n",
    "100, 0.001 = 9.5%\n",
    "30, 0.001 = 17.8%\n",
    "50, 0.001 = 1.0%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 12, 3)\n",
    "\t\tself.fc1 = nn.Linear(47628, 102)\n",
    "30, 0.001 = 19.9%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.fc1 = nn.Linear(23814, 102)\n",
    "30, 0.001 = 17.9%\n",
    "15, 0.001 = 16.0%\n",
    "10, 0.001 = 19.0%\n",
    "5, 0.001 = 14.4%\n",
    "\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.fc1 = nn.Linear(23814, 102)\n",
    "30, 0.001 = 19.2%\n",
    "10, 0.001 = 16.8%\n",
    "50, 0.001 = 17.0\n",
    "\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.conv4 = nn.Conv2d(24, 48, 3)\n",
    "\t\tself.conv5 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(384, 102)\n",
    "30, 0.001 = 18.6%\n",
    "\n",
    "MORE\n",
    "self.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.fc1 = nn.Linear(21600, 102)\n",
    "\tdef forward(self, x):\n",
    "\t\tx = (F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = self.fc1(x)\n",
    "        5 epochs = 10.5%\n",
    "        10 epochs = 17.2%\n",
    "        30 epochs = 16.1%\n",
    "        15 epochs = 10.9\n",
    "        \n",
    "        Same as above but kernel size 4 and self.fc1 = nn.Linear(18816, 102)\n",
    "        10 epochs = 9.2%\n",
    "        15 epochs = 19.2%\n",
    "        20 epochs = 21.5%\n",
    "        25 epochs = 14.5%\n",
    "        30 epochs = 18.4%\n",
    "        \n",
    "        Same as above but kernal size 8 and self.fc1 = nn.Linear(15000, 102)\n",
    "        20 epochs = 15.4 %\n",
    "        30 epochs = 25.5%\n",
    "        35 epochs = 16.1%\n",
    "        \n",
    "        Same as above but with batch normilization after conv1\n",
    "        20 epochs = 12.9%\n",
    "        25 epochs = 25.0%\n",
    "        30 epochs = 19.6%\n",
    "        40 epochs = 18.0%\n",
    "        \n",
    "        Same as above but with dropout of 0.5 after conv1 and batch normilization\n",
    "        10 epochs = 7.3%\n",
    "        15 epochs = 9.1%\n",
    "        20 epochs = 12.4%\n",
    "        30 epochs = 10.3%\n",
    "        40 epochs = 11.6%\n",
    "        \n",
    "        Same as above but dropout set to 0.2\n",
    "        20 epochs = 10.5%\n",
    "        25 epochs = 16.2%\n",
    "        30 epochs = 16.7%\n",
    "        32 epochs = 12.6%\n",
    "        35 epochs = 19.1%\n",
    "        40 epochs = 13.5%\n",
    "        50 epochs = 9.3%\n",
    "        \n",
    "        Now have implemented testing per epoch, now will just record the peak epoch.\n",
    "\n",
    "        Same as above but with conv3 removed and self.fc1 = nn.Linear(8112, 102)\n",
    "        50 epochs = 28.0%\n",
    "        \n",
    "        Same as above but modified conv2 to have 6 outputs and nn.Linear(4056, 102)\n",
    "        20 epochs = 20.0%\n",
    "        Reverting to previous conv2 and fcl\n",
    "        \n",
    "        Now reverted, applying data augmentation: horizontal flip, vertical flip, 90 degree rotates, -90 degree rotates\n",
    "        \n",
    "        Turns out it wasn't properly applying augmentations, I'm using less exact values for the randomness so it basically always does it.\n",
    "        4 epochs = 32.5%\n",
    "\n",
    "        Same as above but without the vertical flipping\n",
    "        5 epochs = 30.9%\n",
    "        It seems vertical flipping is beneficial\n",
    "\n",
    "        Same as before but with vertical flipping back and additionally 45 and -45 rotation augments\n",
    "        4 epochs = 32.3%\n",
    "\n",
    "        Same as before but removing both 45 degree rotation augments and setting dropout rate at 0.3\n",
    "        3 epochs = 29%\n",
    "\n",
    "        Same as before but setting dropout rate to 0.1\n",
    "        2 epochs = 20.2%\n",
    "\n",
    "        Same as before but setting dropout rate to 0.25\n",
    "        6 epochs = 26.7%\n",
    "\n",
    "        Restting dropout rate back to 0.2 and now testing it on the training data to see if the loss function is the issue\n",
    "        6 epochs 31.4%\n",
    "        Loss function is maybe fine\n",
    "\n",
    "        same as before but removing batch normilization\n",
    "        9 epochs = 31.7%\n",
    "        without normilization accuracy increases slower but is more consistently high, will continue without it for now\n",
    "\n",
    "        Same as before but pool kernal of 4 self.fc1 = nn.Linear(10092, 102)\n",
    "        20 epochs = 30.8%\n",
    "        Current model as follows:\n",
    "        self.pool = nn.MaxPool2d(8, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\t#self.batchnorm1 = nn.BatchNorm2d(6)\n",
    "\t\tself.dropout = nn.Dropout2d(0.2)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\t#self.conv3 = nn.Conv2d(3, 24, 3)\n",
    "\t\tself.fc1 = nn.Linear(8112, 102)\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\t#x = self.batchnorm1(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = self.fc1(x)\n",
    "\n",
    "        Massive change, new model below\n",
    "        def __init__(self):\n",
    "\t\tsuper(F102Classifier, self).__init__()\n",
    "\t\t\n",
    "\t\tself.pool = nn.MaxPool2d(8, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3) #3 inputs 6 hiddens\n",
    "\t\tself.batchnorm1 = nn.BatchNorm2d(6) #Normalizes above \n",
    "\t\tself.conv2 = nn.Conv2d(6, 30, 3) # 12 hiddens\n",
    "\t\tself.batchnorm2 = nn.BatchNorm2d(30)\n",
    "\t\tself.conv3 = nn.Conv2d(30, 30, 9) # 12 hiddens\n",
    "\t\tself.batchnorm3 = nn.BatchNorm2d(30)\n",
    "\t\tself.fc1 = nn.Linear(1080, 204)\n",
    "\t\tself.dropout = nn.Dropout(0.2)\n",
    "\t\tself.fc2 = nn.Linear(204, 102) #102 output neurons\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.batchnorm1(x)\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.batchnorm2(x)\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = self.batchnorm3(x)\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "        13 epochs = 34.4%\n",
    "\n",
    "        Same as above but with conv2 output channels to 12\n",
    "        \n",
    " \"\"\"\n",
    "save_path = \"./models/classifier.pth\"\n",
    "torch.save(net.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
