{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 102Flowers Image Classifier\n",
    "\n",
    "This is the main notebook for the project. See the associated report (WIP) for more information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK IN PROGRESS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import argmax\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torchvision.transforms as trnfrm\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import string\n",
    "import random\n",
    "import statistics\n",
    "from statistics import mean\n",
    "from statistics import stdev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set hyperparameters.\n",
    "training_batch_size = 20 #1020 in dataset split\n",
    "validation_batch_size = training_batch_size #1020 in dataset split\n",
    "test_batch_size = training_batch_size #6149 in dataset split\n",
    "epochs = 690 #Nice\n",
    "learning_rate = 0.005 #0.001 for GEU and TMK\n",
    "lambda1 = 0.0001 #Same for DHX, GEU and TMK\n",
    "weightDecay = 0.0005 #0.0001 for GEU and TMK\n",
    "moment = 0.9 #Same for DHX, GEU and TMK\n",
    "crop_size = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Switch to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "\tprint(f\"Found {torch.cuda.device_count()} GPUs. Using cuda:0.\")\n",
    "\tdevice = torch.device(\"cuda:0\")\n",
    "else:\n",
    "\tprint(\"No GPUs found, using CPU.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "h_flipped_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.99),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "v_flipped_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(0.99),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "pos_rotate_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomRotation([30,60]),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "minus_rotate_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomRotation([-60,-30]),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "skewed1_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomPerspective(p = 0.99),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "skewed2_training_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"train\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomPerspective(0.3,p = 0.99),\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "augmented_training_data = ConcatDataset([original_training_data,h_flipped_training_data,v_flipped_training_data,pos_rotate_training_data,minus_rotate_training_data,skewed1_training_data,skewed2_training_data])\n",
    "\n",
    "validation_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"val\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "testing_data = datasets.Flowers102(\n",
    "    root = \"data\",\n",
    "    split = \"test\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(crop_size),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(augmented_training_data, batch_size=training_batch_size, shuffle=True)\n",
    "#train_dataloader = DataLoader(original_training_data, batch_size=training_batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=validation_batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = F.one_hot(torch.tensor([e for e in range(0,102)], device=\"cuda:0\"), num_classes=102)\n",
    "classifications, classifications.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F102Classifier(nn.Module):\n",
    "    #\"\"\"Dxh/Geu \n",
    "    def __init__(self):\n",
    "        super(F102Classifier, self).__init__()    \n",
    "        self.pool = nn.MaxPool2d(3, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, bias = False, groups = 1) #3 inputs channels\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32) #Normalizes above \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, bias = False, groups = 1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, bias = False, groups = 1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 96, 3, bias = False, groups = 1)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(96)\n",
    "        self.conv5 = nn.Conv2d(96, 96, 3, bias = False, groups = 1)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(96)\n",
    "        self.fc1 = nn.Linear(2400, 2400)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(2400, 102) #102 output neurons\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = self.batchnorm5(x)\n",
    "        x = x.view(training_batch_size, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    #\"\"\"\n",
    "    \"\"\" Tmk\n",
    "    def __init__(self):\n",
    "        super(F102Classifier, self).__init__() \n",
    "        self.pool = nn.MaxPool2d(2, 2) #compression kernal using max values\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, bias = False, groups = 1) #3 inputs channels\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32) #Normalizes above \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, bias = False, groups = 1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, bias = False, groups = 1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, bias = False, groups = 1)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 64, 3, bias = False, groups = 1)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(2304, 2304)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(2304, 102) #102 output neurons\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = self.batchnorm5(x)\n",
    "        x = x.view(training_batch_size, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    #\"\"\"\n",
    "\n",
    "    def lambda1_loss(self, n):\n",
    "      return torch.abs(n).sum()\n",
    "    \n",
    "net = F102Classifier()\n",
    "\n",
    "if device == torch.device(\"cuda:0\"):\n",
    "    net.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function & Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "#optimiser = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weightDecay,)\n",
    "optimiser = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=weightDecay, momentum=moment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    avgLoss = 0\n",
    "    for batch, (i,j) in enumerate(dataloader):\n",
    "\n",
    "        features, labels = i.to(device), j.to(device)\n",
    "\n",
    "        l1_parameters = []\n",
    "        for parameter in model.parameters():\n",
    "          l1_parameters.append(parameter.view(-1))\n",
    "        l1_regularization = lambda1 * model.lambda1_loss(torch.cat(l1_parameters))\n",
    "\n",
    "        # Compute the loss based off the predictions vs labels\n",
    "        predictions = model(features)\n",
    "        loss = loss_fn(predictions, labels) + l1_regularization\n",
    "        avgLoss += loss\n",
    "\n",
    "        #Compute back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch+1) % len(dataloader) == 0: #after final batch of each epoch\n",
    "            avgLoss = avgLoss / training_batch_size\n",
    "            #print(\"lamda1: \" + str(l1_regularization))\n",
    "            print(f'Average Training Loss: {loss.item()}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn): #Used whilst training\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    for batch, (i,j) in enumerate(dataloader):\n",
    "        features, labels = i.to(device), j.to(device)\n",
    "        model.cuda()\n",
    "        pred = model(features)\n",
    "        test_loss += loss_fn(pred, labels).item()\n",
    "        correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= (num_batches * training_batch_size)\n",
    "\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss : {test_loss:>8f} \\n\")\n",
    "    return 100*correct, test_loss\n",
    "\n",
    "def testDSA(dataloader, model, loss_fn): #DatasetSizeAgnostic, needed as testing dataset has no compatible batch sizes\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader) - 1\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    n = 1\n",
    "    for batch, (i,j) in enumerate(dataloader):\n",
    "        if n <= num_batches:\n",
    "            features, labels = i.to(device), j.to(device)\n",
    "            model.cuda()\n",
    "            pred = model(features)\n",
    "            test_loss += loss_fn(pred, labels).item()\n",
    "            correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n",
    "            n += 1\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= (num_batches * training_batch_size)\n",
    "\n",
    "    return 100*correct, test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training and Testing And saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "best = 0\n",
    "upperLetters = string.ascii_uppercase\n",
    "lowerLetters = string.ascii_lowercase\n",
    "name = random.choice(upperLetters) + random.choice(lowerLetters) + random.choice(lowerLetters)\n",
    "print(\"Random Name of Model is \" + name)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}-------------')\n",
    "    train(train_dataloader, net, loss_function, optimiser)\n",
    "    #print(\"Training Test\")\n",
    "    #test(train_dataloader, net, loss_function)\n",
    "    print(\"Validation Test\")\n",
    "    current = test(validation_dataloader, net, loss_function)\n",
    "    if current > best:\n",
    "        print(\"new best, saving model \" + name + \" for epoch \" + str(t+1))\n",
    "        best = current\n",
    "        save_path = \"./models/classifier\" + name+  \"Epoch\" + str(t+1) + \"Accuracy\" + str(int(100*best))+ \".pth\"\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "print('Finished Training, Testing and Saving')\n",
    "#\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Keeping and Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" 30,0.001 = 15.2%\n",
    " 100, 0.001 = 14.3%\n",
    " 50, 0.001 = 16.4%\n",
    " 30,0.01 = 1.0%  Herma-OF\"\"\"\n",
    "\n",
    "\"\"\"For Theo\t\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3) \n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.conv4 = nn.Conv2d(24, 48, 3)\n",
    "\t\tself.conv5 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(384, 1024)\n",
    "\t\tself.fc2 = nn.Linear(1024, 512)\n",
    "\t\tself.fc3 = nn.Linear(512, 102)\t\n",
    "30, 0.01 = 1.0%\n",
    "30, 0.001 = 14.0%\n",
    "100, 0.001 = 13.2%\n",
    "30, 0.0001 = 7.2%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 12, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(12, 48, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(18816, 1024)\n",
    "\t\tself.fc2 = nn.Linear(1024, 512)\n",
    "\t\tself.fc3 = nn.Linear(512, 102)\t\t\n",
    "100, 0.001 = 9.5%\n",
    "30, 0.001 = 17.8%\n",
    "50, 0.001 = 1.0%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 12, 3)\n",
    "\t\tself.fc1 = nn.Linear(47628, 102)\n",
    "30, 0.001 = 19.9%\n",
    "\n",
    "\t\tself.pool = nn.AvgPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.fc1 = nn.Linear(23814, 102)\n",
    "30, 0.001 = 17.9%\n",
    "15, 0.001 = 16.0%\n",
    "10, 0.001 = 19.0%\n",
    "5, 0.001 = 14.4%\n",
    "\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.fc1 = nn.Linear(23814, 102)\n",
    "30, 0.001 = 19.2%\n",
    "10, 0.001 = 16.8%\n",
    "50, 0.001 = 17.0\n",
    "\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.conv4 = nn.Conv2d(24, 48, 3)\n",
    "\t\tself.conv5 = nn.Conv2d(48, 96, 3)\n",
    "\t\tself.fc1 = nn.Linear(384, 102)\n",
    "30, 0.001 = 18.6%\n",
    "\n",
    "MORE\n",
    "self.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\tself.conv3 = nn.Conv2d(12, 24, 3)\n",
    "\t\tself.fc1 = nn.Linear(21600, 102)\n",
    "\tdef forward(self, x):\n",
    "\t\tx = (F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = self.fc1(x)\n",
    "        5 epochs = 10.5%\n",
    "        10 epochs = 17.2%\n",
    "        30 epochs = 16.1%\n",
    "        15 epochs = 10.9\n",
    "        \n",
    "        Same as above but kernel size 4 and self.fc1 = nn.Linear(18816, 102)\n",
    "        10 epochs = 9.2%\n",
    "        15 epochs = 19.2%\n",
    "        20 epochs = 21.5%\n",
    "        25 epochs = 14.5%\n",
    "        30 epochs = 18.4%\n",
    "        \n",
    "        Same as above but kernal size 8 and self.fc1 = nn.Linear(15000, 102)\n",
    "        20 epochs = 15.4 %\n",
    "        30 epochs = 25.5%\n",
    "        35 epochs = 16.1%\n",
    "        \n",
    "        Same as above but with batch normilization after conv1\n",
    "        20 epochs = 12.9%\n",
    "        25 epochs = 25.0%\n",
    "        30 epochs = 19.6%\n",
    "        40 epochs = 18.0%\n",
    "        \n",
    "        Same as above but with dropout of 0.5 after conv1 and batch normilization\n",
    "        10 epochs = 7.3%\n",
    "        15 epochs = 9.1%\n",
    "        20 epochs = 12.4%\n",
    "        30 epochs = 10.3%\n",
    "        40 epochs = 11.6%\n",
    "        \n",
    "        Same as above but dropout set to 0.2\n",
    "        20 epochs = 10.5%\n",
    "        25 epochs = 16.2%\n",
    "        30 epochs = 16.7%\n",
    "        32 epochs = 12.6%\n",
    "        35 epochs = 19.1%\n",
    "        40 epochs = 13.5%\n",
    "        50 epochs = 9.3%\n",
    "        \n",
    "        Now have implemented testing per epoch, now will just record the peak epoch.\n",
    "\n",
    "        Same as above but with conv3 removed and self.fc1 = nn.Linear(8112, 102)\n",
    "        50 epochs = 28.0%\n",
    "        \n",
    "        Same as above but modified conv2 to have 6 outputs and nn.Linear(4056, 102)\n",
    "        20 epochs = 20.0%\n",
    "        Reverting to previous conv2 and fcl\n",
    "        \n",
    "        Now reverted, applying data augmentation: horizontal flip, vertical flip, 90 degree rotates, -90 degree rotates\n",
    "        \n",
    "        Turns out it wasn't properly applying augmentations, I'm using less exact values for the randomness so it basically always does it.\n",
    "        4 epochs = 32.5%\n",
    "\n",
    "        Same as above but without the vertical flipping\n",
    "        5 epochs = 30.9%\n",
    "        It seems vertical flipping is beneficial\n",
    "\n",
    "        Same as before but with vertical flipping back and additionally 45 and -45 rotation augments\n",
    "        4 epochs = 32.3%\n",
    "\n",
    "        Same as before but removing both 45 degree rotation augments and setting dropout rate at 0.3\n",
    "        3 epochs = 29%\n",
    "\n",
    "        Same as before but setting dropout rate to 0.1\n",
    "        2 epochs = 20.2%\n",
    "\n",
    "        Same as before but setting dropout rate to 0.25\n",
    "        6 epochs = 26.7%\n",
    "\n",
    "        Restting dropout rate back to 0.2 and now testing it on the training data to see if the loss function is the issue\n",
    "        6 epochs 31.4%\n",
    "        Loss function is maybe fine\n",
    "\n",
    "        same as before but removing batch normilization\n",
    "        9 epochs = 31.7%\n",
    "        without normilization accuracy increases slower but is more consistently high, will continue without it for now\n",
    "\n",
    "        Same as before but pool kernal of 4 self.fc1 = nn.Linear(10092, 102)\n",
    "        20 epochs = 30.8%\n",
    "        Current model as follows:\n",
    "        self.pool = nn.MaxPool2d(8, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3)\n",
    "\t\t#self.batchnorm1 = nn.BatchNorm2d(6)\n",
    "\t\tself.dropout = nn.Dropout2d(0.2)\n",
    "\t\tself.conv2 = nn.Conv2d(6, 12, 3)\n",
    "\t\t#self.conv3 = nn.Conv2d(3, 24, 3)\n",
    "\t\tself.fc1 = nn.Linear(8112, 102)\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\t#x = self.batchnorm1(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = self.fc1(x)\n",
    "\n",
    "        Massive change, new model below\n",
    "        def __init__(self):\n",
    "\t\tsuper(F102Classifier, self).__init__()\n",
    "\t\t\n",
    "\t\tself.pool = nn.MaxPool2d(8, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 6, 3) #3 inputs 6 hiddens\n",
    "\t\tself.batchnorm1 = nn.BatchNorm2d(6) #Normalizes above \n",
    "\t\tself.conv2 = nn.Conv2d(6, 30, 3) # 12 hiddens\n",
    "\t\tself.batchnorm2 = nn.BatchNorm2d(30)\n",
    "\t\tself.conv3 = nn.Conv2d(30, 30, 9) # 12 hiddens\n",
    "\t\tself.batchnorm3 = nn.BatchNorm2d(30)\n",
    "\t\tself.fc1 = nn.Linear(1080, 204)\n",
    "\t\tself.dropout = nn.Dropout(0.2)\n",
    "\t\tself.fc2 = nn.Linear(204, 102) #102 output neurons\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.batchnorm1(x)\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.batchnorm2(x)\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = self.batchnorm3(x)\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "        14 epochs = 42.5%\n",
    "        \n",
    "        Experimented with different crop size, 128 seems best. Also now are saving the models of the best epochs so good ones are not lost\n",
    "\n",
    "\t\timplementing random names, L1 and L2 regularisation\n",
    "\t\tSck epoch 13, accuracy = 45%\n",
    "\n",
    "\t\timplementing additional augmentation with random perspectives\n",
    "\n",
    "\t\tRecognising the importance of high channel outputs, lol\n",
    "\t\t\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1 = nn.Conv2d(3, 32, 3, bias = False, groups = 1) #3 inputs 6 hiddens\n",
    "\t\tself.batchnorm1 = nn.BatchNorm2d(32) #Normalizes above \n",
    "\t\tself.conv2 = nn.Conv2d(32, 64, 3, bias = False, groups = 1) # 12 hiddens\n",
    "\t\tself.batchnorm2 = nn.BatchNorm2d(64)\n",
    "\t\tself.conv3 = nn.Conv2d(64, 64, 3, bias = False, groups = 1) # 12 hiddens\n",
    "\t\tself.batchnorm3 = nn.BatchNorm2d(64)\n",
    "\t\tself.conv4 = nn.Conv2d(64, 64, 3, bias = False, groups = 1) # 12 hiddens\n",
    "\t\tself.batchnorm4 = nn.BatchNorm2d(64)\n",
    "\t\tself.conv5 = nn.Conv2d(64, 64, 3, bias = False, groups = 1) # 12 hiddens\n",
    "\t\tself.batchnorm5 = nn.BatchNorm2d(64)\n",
    "\t\tself.fc1 = nn.Linear(2304, 2304)\n",
    "\t\tself.dropout = nn.Dropout(0.2)\n",
    "\t\tself.fc2 = nn.Linear(2304, 102) #102 output neurons\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(self.conv1(x))\n",
    "\t\tx = self.batchnorm1(x)\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.batchnorm2(x)\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = self.batchnorm3(x)\n",
    "\t\tx = self.pool(F.relu(self.conv4(x)))\n",
    "\t\tx = self.batchnorm4(x)\n",
    "\t\tx = self.pool(F.relu(self.conv5(x)))\n",
    "\t\tx = self.batchnorm5(x)\n",
    "\t\tx = x.view(training_batch_size, -1)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\t#x = self.dropout(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "\t\tlearning_rate = 0.001\n",
    "\t\tlambda1 = 0.001\n",
    "\t\tweightDecay = 0.001\n",
    "\t\t#momentum = 0.9\n",
    "\t\tcrop_size = 128\n",
    "\n",
    "\t\tTmk shown in discord has 47% (peak of 55% but took a while)\n",
    "\t\tExx has 128 output channels in final layers but seems to overfit more, 44%\n",
    "\t\tQba is Exx but with dropout, accuracy is increasing and overfitting less, 50%\n",
    "\n",
    "\t \tDxh is based of Geu but uses SGD optimiser rather than ADAM, achieves 58% but seems to still be overfitting\n",
    "\t\tHwx is based off Dxh but has quntiuple lambda1 value from 0.0001\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "modelName = \"Dxh\"\n",
    "modelEpoch = 60\n",
    "modelAccuracy = 58\n",
    "modelpath = modelName + \"Epoch\" + str(modelEpoch) + \"Accuracy\" + str(modelAccuracy)\n",
    "\n",
    "testModel = F102Classifier()\n",
    "testModel.load_state_dict(torch.load(\"./models/classifier\" + modelpath + \".pth\"))\n",
    "\n",
    "def meanRepeatedTests(dataloader, testModel, loss_function, n = 5):\n",
    "    output = [0,0]\n",
    "    for i in range(n):\n",
    "        results = testDSA(validation_dataloader, testModel, loss_function)\n",
    "        output[0] += results[0] / n\n",
    "        output[1] += results[1] / n\n",
    "    return output\n",
    "\n",
    "print(\"Test Results for \" + modelpath)\n",
    "print(\"\\nTraining Data\")\n",
    "test(train_dataloader, testModel, loss_function)\n",
    "print(\"\\nValidation Data\")\n",
    "test(validation_dataloader, testModel, loss_function)\n",
    "print(\"\\nTesting Data\")\n",
    "output = meanRepeatedTests(test_dataloader,testModel, loss_function)\n",
    "print(f\"Accuracy: {(output[0]):>0.1f}%, Avg loss : {output[1]:>8f} \\n\")\n",
    "#\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
